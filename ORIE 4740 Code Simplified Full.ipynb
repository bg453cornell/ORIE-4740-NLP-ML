{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import os\n",
    "import struct\n",
    "import nltk\n",
    "#nltk.download() \n",
    "from nltk.tag import pos_tag # pos_tag shows the characteristic of a word \n",
    "                             # using information in the database\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: empath in c:\\programdata\\anaconda2\\lib\\site-packages\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda2\\lib\\site-packages (from empath)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install Empath tool for analyzing text across lexical categories\n",
    "import sys\n",
    "!{sys.executable} -m pip install empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from empath import Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 1 Data Preparation\n",
    "### 1.1 Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Combined_News_DJIA.csv\", header = 0, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>Top9</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-06-27</th>\n",
       "      <td>0</td>\n",
       "      <td>Barclays and RBS shares suspended from trading...</td>\n",
       "      <td>Pope says Church should ask forgiveness from g...</td>\n",
       "      <td>Poland 'shocked' by xenophobic abuse of Poles ...</td>\n",
       "      <td>There will be no second referendum, cabinet ag...</td>\n",
       "      <td>Scotland welcome to join EU, Merkel ally says</td>\n",
       "      <td>Sterling dips below Friday's 31-year low amid ...</td>\n",
       "      <td>No negative news about South African President...</td>\n",
       "      <td>Surge in Hate Crimes in the U.K. Following U.K...</td>\n",
       "      <td>Weapons shipped into Jordan by the CIA and Sau...</td>\n",
       "      <td>...</td>\n",
       "      <td>German lawyers to probe Erdogan over alleged w...</td>\n",
       "      <td>Boris Johnson says the UK will continue to \"in...</td>\n",
       "      <td>Richard Branson is calling on the UK governmen...</td>\n",
       "      <td>Turkey 'sorry for downing Russian jet'</td>\n",
       "      <td>Edward Snowden lawyer vows new push for pardon...</td>\n",
       "      <td>Brexit opinion poll reveals majority don't wan...</td>\n",
       "      <td>Conservative MP Leave Campaigner: \"The leave c...</td>\n",
       "      <td>Economists predict UK recession, further weake...</td>\n",
       "      <td>New EU 'superstate plan by France, Germany: Cr...</td>\n",
       "      <td>Pakistani clerics declare transgender marriage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-28</th>\n",
       "      <td>1</td>\n",
       "      <td>2,500 Scientists To Australia: If You Want To ...</td>\n",
       "      <td>The personal details of 112,000 French police ...</td>\n",
       "      <td>S&amp;amp;P cuts United Kingdom sovereign credit r...</td>\n",
       "      <td>Huge helium deposit found in Africa</td>\n",
       "      <td>CEO of the South African state broadcaster qui...</td>\n",
       "      <td>Brexit cost investors $2 trillion, the worst o...</td>\n",
       "      <td>Hong Kong democracy activists call for return ...</td>\n",
       "      <td>Brexit: Iceland president says UK can join 'tr...</td>\n",
       "      <td>UK's Osborne: 'Absolutely' going to have to cu...</td>\n",
       "      <td>...</td>\n",
       "      <td>US, Canada and Mexico pledge 50% of power from...</td>\n",
       "      <td>There is increasing evidence that Australia is...</td>\n",
       "      <td>Richard Branson, the founder of Virgin Group, ...</td>\n",
       "      <td>37,000-yr-old skull from Borneo reveals surpri...</td>\n",
       "      <td>Palestinians stone Western Wall worshipers; po...</td>\n",
       "      <td>Jean-Claude Juncker asks Farage: Why are you h...</td>\n",
       "      <td>\"Romanians for Remainians\" offering a new home...</td>\n",
       "      <td>Brexit: Gibraltar in talks with Scotland to st...</td>\n",
       "      <td>8 Suicide Bombers Strike Lebanon</td>\n",
       "      <td>Mexico's security forces routinely use 'sexual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-29</th>\n",
       "      <td>1</td>\n",
       "      <td>Explosion At Airport In Istanbul</td>\n",
       "      <td>Yemeni former president: Terrorism is the offs...</td>\n",
       "      <td>UK must accept freedom of movement to access E...</td>\n",
       "      <td>Devastated: scientists too late to captive bre...</td>\n",
       "      <td>British Labor Party leader Jeremy Corbyn loses...</td>\n",
       "      <td>A Muslim Shop in the UK Was Just Firebombed Wh...</td>\n",
       "      <td>Mexican Authorities Sexually Torture Women in ...</td>\n",
       "      <td>UK shares and pound continue to recover</td>\n",
       "      <td>Iceland historian Johannesson wins presidentia...</td>\n",
       "      <td>...</td>\n",
       "      <td>Escape Tunnel, Dug by Hand, Is Found at Holoca...</td>\n",
       "      <td>The land under Beijing is sinking by as much a...</td>\n",
       "      <td>Car bomb and Anti-Islamic attack on Mosque in ...</td>\n",
       "      <td>Emaciated lions in Taiz Zoo are trapped in blo...</td>\n",
       "      <td>Rupert Murdoch describes Brexit as 'wonderful'...</td>\n",
       "      <td>More than 40 killed in Yemen suicide attacks</td>\n",
       "      <td>Google Found Disastrous Symantec and Norton Vu...</td>\n",
       "      <td>Extremist violence on the rise in Germany: Dom...</td>\n",
       "      <td>BBC News: Labour MPs pass Corbyn no-confidence...</td>\n",
       "      <td>Tiny New Zealand town with 'too many jobs' lau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>1</td>\n",
       "      <td>Jamaica proposes marijuana dispensers for tour...</td>\n",
       "      <td>Stephen Hawking says pollution and 'stupidity'...</td>\n",
       "      <td>Boris Johnson says he will not run for Tory pa...</td>\n",
       "      <td>Six gay men in Ivory Coast were abused and for...</td>\n",
       "      <td>Switzerland denies citizenship to Muslim immig...</td>\n",
       "      <td>Palestinian terrorist stabs israeli teen girl ...</td>\n",
       "      <td>Puerto Rico will default on $1 billion of debt...</td>\n",
       "      <td>Republic of Ireland fans to be awarded medal f...</td>\n",
       "      <td>Afghan suicide bomber 'kills up to 40' - BBC News</td>\n",
       "      <td>...</td>\n",
       "      <td>Googles free wifi at Indian railway stations i...</td>\n",
       "      <td>Mounting evidence suggests 'hobbits' were wipe...</td>\n",
       "      <td>The men who carried out Tuesday's terror attac...</td>\n",
       "      <td>Calls to suspend Saudi Arabia from UN Human Ri...</td>\n",
       "      <td>More Than 100 Nobel Laureates Call Out Greenpe...</td>\n",
       "      <td>British pedophile sentenced to 85 years in US ...</td>\n",
       "      <td>US permitted 1,200 offshore fracks in Gulf of ...</td>\n",
       "      <td>We will be swimming in ridicule - French beach...</td>\n",
       "      <td>UEFA says no minutes of silence for Istanbul v...</td>\n",
       "      <td>Law Enforcement Sources: Gun Used in Paris Ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01</th>\n",
       "      <td>1</td>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "      <td>Brazil: Huge spike in number of police killing...</td>\n",
       "      <td>Austria's highest court annuls presidential el...</td>\n",
       "      <td>Facebook wins privacy case, can track any Belg...</td>\n",
       "      <td>Switzerland denies Muslim girls citizenship af...</td>\n",
       "      <td>...</td>\n",
       "      <td>The United States has placed Myanmar, Uzbekist...</td>\n",
       "      <td>S&amp;amp;P revises European Union credit rating t...</td>\n",
       "      <td>India gets $1 billion loan from World Bank for...</td>\n",
       "      <td>U.S. sailors detained by Iran spoke too much u...</td>\n",
       "      <td>Mass fish kill in Vietnam solved as Taiwan ste...</td>\n",
       "      <td>Philippines president Rodrigo Duterte urges pe...</td>\n",
       "      <td>Spain arrests three Pakistanis accused of prom...</td>\n",
       "      <td>Venezuela, where anger over food shortages is ...</td>\n",
       "      <td>A Hindu temple worker has been killed by three...</td>\n",
       "      <td>Ozone layer hole seems to be healing - US &amp;amp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Label                                               Top1  \\\n",
       "Date                                                                   \n",
       "2016-06-27      0  Barclays and RBS shares suspended from trading...   \n",
       "2016-06-28      1  2,500 Scientists To Australia: If You Want To ...   \n",
       "2016-06-29      1                   Explosion At Airport In Istanbul   \n",
       "2016-06-30      1  Jamaica proposes marijuana dispensers for tour...   \n",
       "2016-07-01      1  A 117-year-old woman in Mexico City finally re...   \n",
       "\n",
       "                                                         Top2  \\\n",
       "Date                                                            \n",
       "2016-06-27  Pope says Church should ask forgiveness from g...   \n",
       "2016-06-28  The personal details of 112,000 French police ...   \n",
       "2016-06-29  Yemeni former president: Terrorism is the offs...   \n",
       "2016-06-30  Stephen Hawking says pollution and 'stupidity'...   \n",
       "2016-07-01   IMF chief backs Athens as permanent Olympic host   \n",
       "\n",
       "                                                         Top3  \\\n",
       "Date                                                            \n",
       "2016-06-27  Poland 'shocked' by xenophobic abuse of Poles ...   \n",
       "2016-06-28  S&amp;P cuts United Kingdom sovereign credit r...   \n",
       "2016-06-29  UK must accept freedom of movement to access E...   \n",
       "2016-06-30  Boris Johnson says he will not run for Tory pa...   \n",
       "2016-07-01  The president of France says if Brexit won, so...   \n",
       "\n",
       "                                                         Top4  \\\n",
       "Date                                                            \n",
       "2016-06-27  There will be no second referendum, cabinet ag...   \n",
       "2016-06-28                Huge helium deposit found in Africa   \n",
       "2016-06-29  Devastated: scientists too late to captive bre...   \n",
       "2016-06-30  Six gay men in Ivory Coast were abused and for...   \n",
       "2016-07-01  British Man Who Must Give Police 24 Hours' Not...   \n",
       "\n",
       "                                                         Top5  \\\n",
       "Date                                                            \n",
       "2016-06-27      Scotland welcome to join EU, Merkel ally says   \n",
       "2016-06-28  CEO of the South African state broadcaster qui...   \n",
       "2016-06-29  British Labor Party leader Jeremy Corbyn loses...   \n",
       "2016-06-30  Switzerland denies citizenship to Muslim immig...   \n",
       "2016-07-01  100+ Nobel laureates urge Greenpeace to stop o...   \n",
       "\n",
       "                                                         Top6  \\\n",
       "Date                                                            \n",
       "2016-06-27  Sterling dips below Friday's 31-year low amid ...   \n",
       "2016-06-28  Brexit cost investors $2 trillion, the worst o...   \n",
       "2016-06-29  A Muslim Shop in the UK Was Just Firebombed Wh...   \n",
       "2016-06-30  Palestinian terrorist stabs israeli teen girl ...   \n",
       "2016-07-01  Brazil: Huge spike in number of police killing...   \n",
       "\n",
       "                                                         Top7  \\\n",
       "Date                                                            \n",
       "2016-06-27  No negative news about South African President...   \n",
       "2016-06-28  Hong Kong democracy activists call for return ...   \n",
       "2016-06-29  Mexican Authorities Sexually Torture Women in ...   \n",
       "2016-06-30  Puerto Rico will default on $1 billion of debt...   \n",
       "2016-07-01  Austria's highest court annuls presidential el...   \n",
       "\n",
       "                                                         Top8  \\\n",
       "Date                                                            \n",
       "2016-06-27  Surge in Hate Crimes in the U.K. Following U.K...   \n",
       "2016-06-28  Brexit: Iceland president says UK can join 'tr...   \n",
       "2016-06-29            UK shares and pound continue to recover   \n",
       "2016-06-30  Republic of Ireland fans to be awarded medal f...   \n",
       "2016-07-01  Facebook wins privacy case, can track any Belg...   \n",
       "\n",
       "                                                         Top9  \\\n",
       "Date                                                            \n",
       "2016-06-27  Weapons shipped into Jordan by the CIA and Sau...   \n",
       "2016-06-28  UK's Osborne: 'Absolutely' going to have to cu...   \n",
       "2016-06-29  Iceland historian Johannesson wins presidentia...   \n",
       "2016-06-30  Afghan suicide bomber 'kills up to 40' - BBC News   \n",
       "2016-07-01  Switzerland denies Muslim girls citizenship af...   \n",
       "\n",
       "                                  ...                          \\\n",
       "Date                              ...                           \n",
       "2016-06-27                        ...                           \n",
       "2016-06-28                        ...                           \n",
       "2016-06-29                        ...                           \n",
       "2016-06-30                        ...                           \n",
       "2016-07-01                        ...                           \n",
       "\n",
       "                                                        Top16  \\\n",
       "Date                                                            \n",
       "2016-06-27  German lawyers to probe Erdogan over alleged w...   \n",
       "2016-06-28  US, Canada and Mexico pledge 50% of power from...   \n",
       "2016-06-29  Escape Tunnel, Dug by Hand, Is Found at Holoca...   \n",
       "2016-06-30  Googles free wifi at Indian railway stations i...   \n",
       "2016-07-01  The United States has placed Myanmar, Uzbekist...   \n",
       "\n",
       "                                                        Top17  \\\n",
       "Date                                                            \n",
       "2016-06-27  Boris Johnson says the UK will continue to \"in...   \n",
       "2016-06-28  There is increasing evidence that Australia is...   \n",
       "2016-06-29  The land under Beijing is sinking by as much a...   \n",
       "2016-06-30  Mounting evidence suggests 'hobbits' were wipe...   \n",
       "2016-07-01  S&amp;P revises European Union credit rating t...   \n",
       "\n",
       "                                                        Top18  \\\n",
       "Date                                                            \n",
       "2016-06-27  Richard Branson is calling on the UK governmen...   \n",
       "2016-06-28  Richard Branson, the founder of Virgin Group, ...   \n",
       "2016-06-29  Car bomb and Anti-Islamic attack on Mosque in ...   \n",
       "2016-06-30  The men who carried out Tuesday's terror attac...   \n",
       "2016-07-01  India gets $1 billion loan from World Bank for...   \n",
       "\n",
       "                                                        Top19  \\\n",
       "Date                                                            \n",
       "2016-06-27             Turkey 'sorry for downing Russian jet'   \n",
       "2016-06-28  37,000-yr-old skull from Borneo reveals surpri...   \n",
       "2016-06-29  Emaciated lions in Taiz Zoo are trapped in blo...   \n",
       "2016-06-30  Calls to suspend Saudi Arabia from UN Human Ri...   \n",
       "2016-07-01  U.S. sailors detained by Iran spoke too much u...   \n",
       "\n",
       "                                                        Top20  \\\n",
       "Date                                                            \n",
       "2016-06-27  Edward Snowden lawyer vows new push for pardon...   \n",
       "2016-06-28  Palestinians stone Western Wall worshipers; po...   \n",
       "2016-06-29  Rupert Murdoch describes Brexit as 'wonderful'...   \n",
       "2016-06-30  More Than 100 Nobel Laureates Call Out Greenpe...   \n",
       "2016-07-01  Mass fish kill in Vietnam solved as Taiwan ste...   \n",
       "\n",
       "                                                        Top21  \\\n",
       "Date                                                            \n",
       "2016-06-27  Brexit opinion poll reveals majority don't wan...   \n",
       "2016-06-28  Jean-Claude Juncker asks Farage: Why are you h...   \n",
       "2016-06-29       More than 40 killed in Yemen suicide attacks   \n",
       "2016-06-30  British pedophile sentenced to 85 years in US ...   \n",
       "2016-07-01  Philippines president Rodrigo Duterte urges pe...   \n",
       "\n",
       "                                                        Top22  \\\n",
       "Date                                                            \n",
       "2016-06-27  Conservative MP Leave Campaigner: \"The leave c...   \n",
       "2016-06-28  \"Romanians for Remainians\" offering a new home...   \n",
       "2016-06-29  Google Found Disastrous Symantec and Norton Vu...   \n",
       "2016-06-30  US permitted 1,200 offshore fracks in Gulf of ...   \n",
       "2016-07-01  Spain arrests three Pakistanis accused of prom...   \n",
       "\n",
       "                                                        Top23  \\\n",
       "Date                                                            \n",
       "2016-06-27  Economists predict UK recession, further weake...   \n",
       "2016-06-28  Brexit: Gibraltar in talks with Scotland to st...   \n",
       "2016-06-29  Extremist violence on the rise in Germany: Dom...   \n",
       "2016-06-30  We will be swimming in ridicule - French beach...   \n",
       "2016-07-01  Venezuela, where anger over food shortages is ...   \n",
       "\n",
       "                                                        Top24  \\\n",
       "Date                                                            \n",
       "2016-06-27  New EU 'superstate plan by France, Germany: Cr...   \n",
       "2016-06-28                   8 Suicide Bombers Strike Lebanon   \n",
       "2016-06-29  BBC News: Labour MPs pass Corbyn no-confidence...   \n",
       "2016-06-30  UEFA says no minutes of silence for Istanbul v...   \n",
       "2016-07-01  A Hindu temple worker has been killed by three...   \n",
       "\n",
       "                                                        Top25  \n",
       "Date                                                           \n",
       "2016-06-27  Pakistani clerics declare transgender marriage...  \n",
       "2016-06-28  Mexico's security forces routinely use 'sexual...  \n",
       "2016-06-29  Tiny New Zealand town with 'too many jobs' lau...  \n",
       "2016-06-30  Law Enforcement Sources: Gun Used in Paris Ter...  \n",
       "2016-07-01  Ozone layer hole seems to be healing - US &amp...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1989, 26)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"Georgia \\'downs two Russian warplanes\\' as countries move to brink of war\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Preparation - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "f = open('./data/stopwords.txt', 'r')\n",
    "for l in f.readlines():\n",
    "    stop_words.append(l.replace('\\n', ''))\n",
    "    \n",
    "additional_stop_words = ['t', 'will']\n",
    "stop_words += additional_stop_words\n",
    "# The list \"stop_words\" has 668 words in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _removeNonAscii(s): \n",
    "    return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = text.replace('(ap)', '')\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub('[^a-zA-Z ?!]+', '', text)\n",
    "    text = _removeNonAscii(text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to D:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to D:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    from functools import reduce\n",
    "    text = clean_text(text)    \n",
    "    tokens = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "    tokens = list(reduce(lambda x,y: x+y, tokens))\n",
    "    tokens = list(filter(lambda token: token not in (stop_words + list(punctuation)) , tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize each headline in df\n",
    "for top in df.columns[1:]:\n",
    "    df[top] = df[top].map(lambda d: tokenizer(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenized_df','wb') as f:\n",
    "    pickle.dump(df,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>Top9</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-08-08</th>\n",
       "      <td>0</td>\n",
       "      <td>[georgia, owns, russian, warplanes, countries,...</td>\n",
       "      <td>[breaking, musharraf, impeached]</td>\n",
       "      <td>[russia, today, columns, troops, roll, south, ...</td>\n",
       "      <td>[russian, tanks, moving, capital, south, osset...</td>\n",
       "      <td>[afghan, children, raped, impunity, official, ...</td>\n",
       "      <td>[russian, tanks, entered, south, ossetia, whil...</td>\n",
       "      <td>[breaking, georgia, invades, south, ossetia, r...</td>\n",
       "      <td>[enemy, combatent, trials, sham, salim, haman,...</td>\n",
       "      <td>[georgian, troops, retreat, osettain, capital,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[georgia, invades, south, ossetia, russia, inv...</td>\n",
       "      <td>[al, qaeda, faces, islamist, backlash]</td>\n",
       "      <td>[condoleezza, rice, prevent, israeli, strike, ...</td>\n",
       "      <td>[busy, day, european, union, approved, sanctio...</td>\n",
       "      <td>[georgia, withdraw, soldiers, iraq, help, figh...</td>\n",
       "      <td>[pentagon, thinks, attacking, iran, bad, idea,...</td>\n",
       "      <td>[caucasus, crisis, georgia, invades, south, os...</td>\n",
       "      <td>[indian, shoe, manufactory, series, work]</td>\n",
       "      <td>[visitors, suffering, mental, illnesses, banne...</td>\n",
       "      <td>[help, mexico, kidnapping, surge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-11</th>\n",
       "      <td>1</td>\n",
       "      <td>[america, nato, help, help, help, iraq]</td>\n",
       "      <td>[bush, puts, foot, georgian, conflict]</td>\n",
       "      <td>[jewish, georgian, minister, israeli, training...</td>\n",
       "      <td>[georgian, army, flees, disarray, russians, ad...</td>\n",
       "      <td>[olympic, opening, ceremony, fireworks, faked]</td>\n",
       "      <td>[mossad, fraudulent, zealand, passports, iraq]</td>\n",
       "      <td>[russia, angered, israeli, military, sale, geo...</td>\n",
       "      <td>[american, citizen, living, ossetia, blames, g...</td>\n",
       "      <td>[war, iv, high, definition]</td>\n",
       "      <td>...</td>\n",
       "      <td>[israel, georgian, aggression]</td>\n",
       "      <td>[tv, russian, georgian, victims]</td>\n",
       "      <td>[riots, going, montreal, canada, police, murde...</td>\n",
       "      <td>[china, overtake, largest, manufacturer]</td>\n",
       "      <td>[war, south, ossetia, pics]</td>\n",
       "      <td>[israeli, physicians, group, condemns, state, ...</td>\n",
       "      <td>[russia, beaten, united, states, head, peak, oil]</td>\n",
       "      <td>[question, georgia, russia, conflict]</td>\n",
       "      <td>[russia, better, war]</td>\n",
       "      <td>[trading, sex, food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-12</th>\n",
       "      <td>0</td>\n",
       "      <td>[member, adorable, year, sang, opening, ceremo...</td>\n",
       "      <td>[russia, ends, georgia, operation]</td>\n",
       "      <td>[sexual, harassment, children]</td>\n",
       "      <td>[al, qa, eda, losing, support, iraq, brutal, c...</td>\n",
       "      <td>[ceasefire, georgia, putin, outmaneuvers, west]</td>\n",
       "      <td>[microsoft, intel, kill, xo, laptop]</td>\n",
       "      <td>[tratfor, russo, georgian, war, balance, power]</td>\n",
       "      <td>[sense, georgia, russia, war, vote, georgia, s...</td>\n",
       "      <td>[military, surprised, timing, swiftness, russi...</td>\n",
       "      <td>...</td>\n",
       "      <td>[troops, georgia, georgia, place]</td>\n",
       "      <td>[russias, response, georgia]</td>\n",
       "      <td>[gorbachev, accuses, making, serious, blunder,...</td>\n",
       "      <td>[russia, georgia, nato, cold, war]</td>\n",
       "      <td>[member, adorable, year, led, country, war, ba...</td>\n",
       "      <td>[war, georgia, israeli, connection]</td>\n",
       "      <td>[signs, point, encouraging, georgia, invade, s...</td>\n",
       "      <td>[christopher, king, argues, nato, georgian, in...</td>\n",
       "      <td>[america, mexico]</td>\n",
       "      <td>[bbc, news, asia, pacific, extinction, man, cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-13</th>\n",
       "      <td>0</td>\n",
       "      <td>[refuses, israel, weapons, attack, iran, report]</td>\n",
       "      <td>[president, ordered, attack, tskhinvali, capit...</td>\n",
       "      <td>[israel, clears, troops, killed, reuters, came...</td>\n",
       "      <td>[britain, policy, tough, drugs, pointless, civ...</td>\n",
       "      <td>[body, year, trunk, latest, ransom, paid, kidn...</td>\n",
       "      <td>[china, moved, quake, survivors, prefab, homes]</td>\n",
       "      <td>[bush, announces, operation, russia, grill, ye...</td>\n",
       "      <td>[russian, forces, sink, georgian, ships]</td>\n",
       "      <td>[commander, navy, air, reconnaissance, squadro...</td>\n",
       "      <td>...</td>\n",
       "      <td>[elephants, extinct]</td>\n",
       "      <td>[humanitarian, missions, georgia, russia, hits...</td>\n",
       "      <td>[georgia, ddos, sources]</td>\n",
       "      <td>[russian, convoy, heads, georgia, violating, t...</td>\n",
       "      <td>[israeli, defence, minister, strike, iran]</td>\n",
       "      <td>[gorbachev, choice]</td>\n",
       "      <td>[witness, russian, forces, head, tbilisi, brea...</td>\n",
       "      <td>[quarter, russians, blame, conflict, poll]</td>\n",
       "      <td>[georgian, president, military, control, seapo...</td>\n",
       "      <td>[nobel, laureate, aleksander, solzhenitsyn, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-14</th>\n",
       "      <td>1</td>\n",
       "      <td>[experts, admit, legalise, drugs]</td>\n",
       "      <td>[war, south, osetia, pictures, russian, soldier]</td>\n",
       "      <td>[wedish, wrestler, ara, abrahamian, throws, me...</td>\n",
       "      <td>[russia, exaggerated, death, toll, south, osse...</td>\n",
       "      <td>[missile, killed, inside, pakistan, launched, ...</td>\n",
       "      <td>[rushdie, condemns, random, house, refusal, pu...</td>\n",
       "      <td>[poland, agree, missle, defense, deal, interes...</td>\n",
       "      <td>[russians, conquer, tblisi, bet, seriously, bet]</td>\n",
       "      <td>[russia, exaggerating, south, ossetian, death,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[bank, analyst, forecast, georgian, crisis, da...</td>\n",
       "      <td>[georgia, confict, set, russia, relations, yea...</td>\n",
       "      <td>[war, caucasus, product, american, imperial, d...</td>\n",
       "      <td>[media, photos, south, ossetia, georgia, confl...</td>\n",
       "      <td>[georgian, tv, reporter, shot, russian, sniper...</td>\n",
       "      <td>[audi, arabia, mother, moves, block, child, ma...</td>\n",
       "      <td>[taliban, wages, war, humanitarian, aid, workers]</td>\n",
       "      <td>[russia, forget, georgia, territorial, integrity]</td>\n",
       "      <td>[arfur, rebels, accuse, sudan, mounting, major...</td>\n",
       "      <td>[philippines, peace, advocate, muslims, assura...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Label                                               Top1  \\\n",
       "Date                                                                   \n",
       "2008-08-08      0  [georgia, owns, russian, warplanes, countries,...   \n",
       "2008-08-11      1            [america, nato, help, help, help, iraq]   \n",
       "2008-08-12      0  [member, adorable, year, sang, opening, ceremo...   \n",
       "2008-08-13      0   [refuses, israel, weapons, attack, iran, report]   \n",
       "2008-08-14      1                  [experts, admit, legalise, drugs]   \n",
       "\n",
       "                                                         Top2  \\\n",
       "Date                                                            \n",
       "2008-08-08                   [breaking, musharraf, impeached]   \n",
       "2008-08-11             [bush, puts, foot, georgian, conflict]   \n",
       "2008-08-12                 [russia, ends, georgia, operation]   \n",
       "2008-08-13  [president, ordered, attack, tskhinvali, capit...   \n",
       "2008-08-14   [war, south, osetia, pictures, russian, soldier]   \n",
       "\n",
       "                                                         Top3  \\\n",
       "Date                                                            \n",
       "2008-08-08  [russia, today, columns, troops, roll, south, ...   \n",
       "2008-08-11  [jewish, georgian, minister, israeli, training...   \n",
       "2008-08-12                     [sexual, harassment, children]   \n",
       "2008-08-13  [israel, clears, troops, killed, reuters, came...   \n",
       "2008-08-14  [wedish, wrestler, ara, abrahamian, throws, me...   \n",
       "\n",
       "                                                         Top4  \\\n",
       "Date                                                            \n",
       "2008-08-08  [russian, tanks, moving, capital, south, osset...   \n",
       "2008-08-11  [georgian, army, flees, disarray, russians, ad...   \n",
       "2008-08-12  [al, qa, eda, losing, support, iraq, brutal, c...   \n",
       "2008-08-13  [britain, policy, tough, drugs, pointless, civ...   \n",
       "2008-08-14  [russia, exaggerated, death, toll, south, osse...   \n",
       "\n",
       "                                                         Top5  \\\n",
       "Date                                                            \n",
       "2008-08-08  [afghan, children, raped, impunity, official, ...   \n",
       "2008-08-11     [olympic, opening, ceremony, fireworks, faked]   \n",
       "2008-08-12    [ceasefire, georgia, putin, outmaneuvers, west]   \n",
       "2008-08-13  [body, year, trunk, latest, ransom, paid, kidn...   \n",
       "2008-08-14  [missile, killed, inside, pakistan, launched, ...   \n",
       "\n",
       "                                                         Top6  \\\n",
       "Date                                                            \n",
       "2008-08-08  [russian, tanks, entered, south, ossetia, whil...   \n",
       "2008-08-11     [mossad, fraudulent, zealand, passports, iraq]   \n",
       "2008-08-12               [microsoft, intel, kill, xo, laptop]   \n",
       "2008-08-13    [china, moved, quake, survivors, prefab, homes]   \n",
       "2008-08-14  [rushdie, condemns, random, house, refusal, pu...   \n",
       "\n",
       "                                                         Top7  \\\n",
       "Date                                                            \n",
       "2008-08-08  [breaking, georgia, invades, south, ossetia, r...   \n",
       "2008-08-11  [russia, angered, israeli, military, sale, geo...   \n",
       "2008-08-12    [tratfor, russo, georgian, war, balance, power]   \n",
       "2008-08-13  [bush, announces, operation, russia, grill, ye...   \n",
       "2008-08-14  [poland, agree, missle, defense, deal, interes...   \n",
       "\n",
       "                                                         Top8  \\\n",
       "Date                                                            \n",
       "2008-08-08  [enemy, combatent, trials, sham, salim, haman,...   \n",
       "2008-08-11  [american, citizen, living, ossetia, blames, g...   \n",
       "2008-08-12  [sense, georgia, russia, war, vote, georgia, s...   \n",
       "2008-08-13           [russian, forces, sink, georgian, ships]   \n",
       "2008-08-14   [russians, conquer, tblisi, bet, seriously, bet]   \n",
       "\n",
       "                                                         Top9  \\\n",
       "Date                                                            \n",
       "2008-08-08  [georgian, troops, retreat, osettain, capital,...   \n",
       "2008-08-11                        [war, iv, high, definition]   \n",
       "2008-08-12  [military, surprised, timing, swiftness, russi...   \n",
       "2008-08-13  [commander, navy, air, reconnaissance, squadro...   \n",
       "2008-08-14  [russia, exaggerating, south, ossetian, death,...   \n",
       "\n",
       "                                  ...                          \\\n",
       "Date                              ...                           \n",
       "2008-08-08                        ...                           \n",
       "2008-08-11                        ...                           \n",
       "2008-08-12                        ...                           \n",
       "2008-08-13                        ...                           \n",
       "2008-08-14                        ...                           \n",
       "\n",
       "                                                        Top16  \\\n",
       "Date                                                            \n",
       "2008-08-08  [georgia, invades, south, ossetia, russia, inv...   \n",
       "2008-08-11                     [israel, georgian, aggression]   \n",
       "2008-08-12                  [troops, georgia, georgia, place]   \n",
       "2008-08-13                               [elephants, extinct]   \n",
       "2008-08-14  [bank, analyst, forecast, georgian, crisis, da...   \n",
       "\n",
       "                                                        Top17  \\\n",
       "Date                                                            \n",
       "2008-08-08             [al, qaeda, faces, islamist, backlash]   \n",
       "2008-08-11                   [tv, russian, georgian, victims]   \n",
       "2008-08-12                       [russias, response, georgia]   \n",
       "2008-08-13  [humanitarian, missions, georgia, russia, hits...   \n",
       "2008-08-14  [georgia, confict, set, russia, relations, yea...   \n",
       "\n",
       "                                                        Top18  \\\n",
       "Date                                                            \n",
       "2008-08-08  [condoleezza, rice, prevent, israeli, strike, ...   \n",
       "2008-08-11  [riots, going, montreal, canada, police, murde...   \n",
       "2008-08-12  [gorbachev, accuses, making, serious, blunder,...   \n",
       "2008-08-13                           [georgia, ddos, sources]   \n",
       "2008-08-14  [war, caucasus, product, american, imperial, d...   \n",
       "\n",
       "                                                        Top19  \\\n",
       "Date                                                            \n",
       "2008-08-08  [busy, day, european, union, approved, sanctio...   \n",
       "2008-08-11           [china, overtake, largest, manufacturer]   \n",
       "2008-08-12                 [russia, georgia, nato, cold, war]   \n",
       "2008-08-13  [russian, convoy, heads, georgia, violating, t...   \n",
       "2008-08-14  [media, photos, south, ossetia, georgia, confl...   \n",
       "\n",
       "                                                        Top20  \\\n",
       "Date                                                            \n",
       "2008-08-08  [georgia, withdraw, soldiers, iraq, help, figh...   \n",
       "2008-08-11                        [war, south, ossetia, pics]   \n",
       "2008-08-12  [member, adorable, year, led, country, war, ba...   \n",
       "2008-08-13         [israeli, defence, minister, strike, iran]   \n",
       "2008-08-14  [georgian, tv, reporter, shot, russian, sniper...   \n",
       "\n",
       "                                                        Top21  \\\n",
       "Date                                                            \n",
       "2008-08-08  [pentagon, thinks, attacking, iran, bad, idea,...   \n",
       "2008-08-11  [israeli, physicians, group, condemns, state, ...   \n",
       "2008-08-12                [war, georgia, israeli, connection]   \n",
       "2008-08-13                                [gorbachev, choice]   \n",
       "2008-08-14  [audi, arabia, mother, moves, block, child, ma...   \n",
       "\n",
       "                                                        Top22  \\\n",
       "Date                                                            \n",
       "2008-08-08  [caucasus, crisis, georgia, invades, south, os...   \n",
       "2008-08-11  [russia, beaten, united, states, head, peak, oil]   \n",
       "2008-08-12  [signs, point, encouraging, georgia, invade, s...   \n",
       "2008-08-13  [witness, russian, forces, head, tbilisi, brea...   \n",
       "2008-08-14  [taliban, wages, war, humanitarian, aid, workers]   \n",
       "\n",
       "                                                        Top23  \\\n",
       "Date                                                            \n",
       "2008-08-08          [indian, shoe, manufactory, series, work]   \n",
       "2008-08-11              [question, georgia, russia, conflict]   \n",
       "2008-08-12  [christopher, king, argues, nato, georgian, in...   \n",
       "2008-08-13         [quarter, russians, blame, conflict, poll]   \n",
       "2008-08-14  [russia, forget, georgia, territorial, integrity]   \n",
       "\n",
       "                                                        Top24  \\\n",
       "Date                                                            \n",
       "2008-08-08  [visitors, suffering, mental, illnesses, banne...   \n",
       "2008-08-11                              [russia, better, war]   \n",
       "2008-08-12                                  [america, mexico]   \n",
       "2008-08-13  [georgian, president, military, control, seapo...   \n",
       "2008-08-14  [arfur, rebels, accuse, sudan, mounting, major...   \n",
       "\n",
       "                                                        Top25  \n",
       "Date                                                           \n",
       "2008-08-08                  [help, mexico, kidnapping, surge]  \n",
       "2008-08-11                               [trading, sex, food]  \n",
       "2008-08-12  [bbc, news, asia, pacific, extinction, man, cl...  \n",
       "2008-08-13  [nobel, laureate, aleksander, solzhenitsyn, ac...  \n",
       "2008-08-14  [philippines, peace, advocate, muslims, assura...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tokenized_df','rb') as f:\n",
    "    tokenized_df = pickle.load(f)\n",
    "tokenized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Filter headlines by topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Use Empath to filter headlines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(tokenized_df.shape[0]):\n",
    "    for j in range(1, tokenized_df.shape[1]):\n",
    "        if tokenized_df.iloc[i, j]:\n",
    "            topic_scores = lexicon.analyze(' '.join(tokenized_df.iloc[i, j]),\\\n",
    "                                               categories=['economics', 'technology', 'politics'], normalize=True)\n",
    "            if sum(topic_scores.values()) == 0.:\n",
    "                tokenized_df.iloc[i, j] = np.NaN\n",
    "        else:\n",
    "            tokenized_df.iloc[i, j] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>Top9</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-08-08</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[busy, day, european, union, approved, sanctio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-11</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[russia, angered, israeli, military, sale, geo...</td>\n",
       "      <td>[american, citizen, living, ossetia, blames, g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[trading, sex, food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-12</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[al, qa, eda, losing, support, iraq, brutal, c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[microsoft, intel, kill, xo, laptop]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-13</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[britain, policy, tough, drugs, pointless, civ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[commander, navy, air, reconnaissance, squadro...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-14</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[war, caucasus, product, american, imperial, d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[philippines, peace, advocate, muslims, assura...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Label Top1 Top2 Top3  \\\n",
       "Date                               \n",
       "2008-08-08      0  NaN  NaN  NaN   \n",
       "2008-08-11      1  NaN  NaN  NaN   \n",
       "2008-08-12      0  NaN  NaN  NaN   \n",
       "2008-08-13      0  NaN  NaN  NaN   \n",
       "2008-08-14      1  NaN  NaN  NaN   \n",
       "\n",
       "                                                         Top4 Top5  \\\n",
       "Date                                                                 \n",
       "2008-08-08                                                NaN  NaN   \n",
       "2008-08-11                                                NaN  NaN   \n",
       "2008-08-12  [al, qa, eda, losing, support, iraq, brutal, c...  NaN   \n",
       "2008-08-13  [britain, policy, tough, drugs, pointless, civ...  NaN   \n",
       "2008-08-14                                                NaN  NaN   \n",
       "\n",
       "                                            Top6  \\\n",
       "Date                                               \n",
       "2008-08-08                                   NaN   \n",
       "2008-08-11                                   NaN   \n",
       "2008-08-12  [microsoft, intel, kill, xo, laptop]   \n",
       "2008-08-13                                   NaN   \n",
       "2008-08-14                                   NaN   \n",
       "\n",
       "                                                         Top7  \\\n",
       "Date                                                            \n",
       "2008-08-08                                                NaN   \n",
       "2008-08-11  [russia, angered, israeli, military, sale, geo...   \n",
       "2008-08-12                                                NaN   \n",
       "2008-08-13                                                NaN   \n",
       "2008-08-14                                                NaN   \n",
       "\n",
       "                                                         Top8  \\\n",
       "Date                                                            \n",
       "2008-08-08                                                NaN   \n",
       "2008-08-11  [american, citizen, living, ossetia, blames, g...   \n",
       "2008-08-12                                                NaN   \n",
       "2008-08-13                                                NaN   \n",
       "2008-08-14                                                NaN   \n",
       "\n",
       "                                                         Top9  \\\n",
       "Date                                                            \n",
       "2008-08-08                                                NaN   \n",
       "2008-08-11                                                NaN   \n",
       "2008-08-12                                                NaN   \n",
       "2008-08-13  [commander, navy, air, reconnaissance, squadro...   \n",
       "2008-08-14                                                NaN   \n",
       "\n",
       "                                  ...                         Top16 Top17  \\\n",
       "Date                              ...                                       \n",
       "2008-08-08                        ...                           NaN   NaN   \n",
       "2008-08-11                        ...                           NaN   NaN   \n",
       "2008-08-12                        ...                           NaN   NaN   \n",
       "2008-08-13                        ...                           NaN   NaN   \n",
       "2008-08-14                        ...                           NaN   NaN   \n",
       "\n",
       "                                                        Top18  \\\n",
       "Date                                                            \n",
       "2008-08-08                                                NaN   \n",
       "2008-08-11                                                NaN   \n",
       "2008-08-12                                                NaN   \n",
       "2008-08-13                                                NaN   \n",
       "2008-08-14  [war, caucasus, product, american, imperial, d...   \n",
       "\n",
       "                                                        Top19 Top20 Top21  \\\n",
       "Date                                                                        \n",
       "2008-08-08  [busy, day, european, union, approved, sanctio...   NaN   NaN   \n",
       "2008-08-11                                                NaN   NaN   NaN   \n",
       "2008-08-12                                                NaN   NaN   NaN   \n",
       "2008-08-13                                                NaN   NaN   NaN   \n",
       "2008-08-14                                                NaN   NaN   NaN   \n",
       "\n",
       "           Top22 Top23 Top24  \\\n",
       "Date                           \n",
       "2008-08-08   NaN   NaN   NaN   \n",
       "2008-08-11   NaN   NaN   NaN   \n",
       "2008-08-12   NaN   NaN   NaN   \n",
       "2008-08-13   NaN   NaN   NaN   \n",
       "2008-08-14   NaN   NaN   NaN   \n",
       "\n",
       "                                                        Top25  \n",
       "Date                                                           \n",
       "2008-08-08                                                NaN  \n",
       "2008-08-11                               [trading, sex, food]  \n",
       "2008-08-12                                                NaN  \n",
       "2008-08-13                                                NaN  \n",
       "2008-08-14  [philippines, peace, advocate, muslims, assura...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if all top25 headlines are filtered out in each day\n",
    "sum(tokenized_df.isnull().all(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save filtered dataframe\n",
    "with open('filtered_tokenized_df', 'wb') as f:\n",
    "    pickle.dump(tokenized_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sentiment Analysis on Headlines Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('filtered_tokenized_df', 'rb') as f:\n",
    "    filtered_tokenized_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "sia = SIA()\n",
    "result = []\n",
    "\n",
    "for i in range(filtered_tokenized_df.shape[0]):\n",
    "    compound = []\n",
    "    neutral =[]\n",
    "    positive = []\n",
    "    negative = []\n",
    "    list_of_headlines = filtered_tokenized_df.iloc[i, :][filtered_tokenized_df.iloc[i, :].notnull()][1:].tolist()\n",
    "    for headline in list_of_headlines:\n",
    "        pol_score = sia.polarity_scores(' '.join(headline))\n",
    "        compound.append(pol_score['compound'])\n",
    "        neutral.append(pol_score['neu'])\n",
    "        positive.append(pol_score['pos'])\n",
    "        negative.append(pol_score['neg'])\n",
    "    result.append({'compound': np.mean(compound), 'neu': np.mean(neutral), 'pos': np.mean(positive), 'neg': np.mean(negative)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_score_df = pd.DataFrame.from_records(result)\n",
    "sentiment_score_df.set_index(filtered_tokenized_df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_score_df['Label'] = filtered_tokenized_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-08-08</th>\n",
       "      <td>0.20230</td>\n",
       "      <td>0.15600</td>\n",
       "      <td>0.62500</td>\n",
       "      <td>0.21900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-11</th>\n",
       "      <td>-0.14695</td>\n",
       "      <td>0.15100</td>\n",
       "      <td>0.80325</td>\n",
       "      <td>0.04575</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-12</th>\n",
       "      <td>-0.65160</td>\n",
       "      <td>0.43400</td>\n",
       "      <td>0.49950</td>\n",
       "      <td>0.06600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-13</th>\n",
       "      <td>-0.13415</td>\n",
       "      <td>0.23475</td>\n",
       "      <td>0.67050</td>\n",
       "      <td>0.09450</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-14</th>\n",
       "      <td>0.03240</td>\n",
       "      <td>0.17375</td>\n",
       "      <td>0.60600</td>\n",
       "      <td>0.22025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            compound      neg      neu      pos  Label\n",
       "Date                                                  \n",
       "2008-08-08   0.20230  0.15600  0.62500  0.21900      0\n",
       "2008-08-11  -0.14695  0.15100  0.80325  0.04575      1\n",
       "2008-08-12  -0.65160  0.43400  0.49950  0.06600      0\n",
       "2008-08-13  -0.13415  0.23475  0.67050  0.09450      0\n",
       "2008-08-14   0.03240  0.17375  0.60600  0.22025      1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sentiment_score_df', 'wb') as f:\n",
    "    pickle.dump(sentiment_score_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Vocabulary Construction\n",
    "This part contains only functions, which will later be called when training and validating the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The following two functions are used to help extract terminology and meaningful words\n",
    "REFERENCE: http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "'''\n",
    "def extract_candidate_chunks(text, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'):\n",
    "    import itertools, nltk, string\n",
    "\n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
    "                                                    for tagged_sent in tagged_sents))\n",
    "\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    candidates = [' '.join(word for word, pos, chunk in group).lower()\n",
    "                  for key, group in itertools.groupby(all_chunks, lambda (word,pos,chunk): chunk != 'O') if key]\n",
    "\n",
    "    return [cand for cand in candidates\n",
    "            if cand not in stop_words and not all(char in punct for char in cand)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Input a ordered word list, output its vocabulary set\n",
    "'''\n",
    "def buildVocabulary(OrderedWord):\n",
    "    # Single words that we think may be meaningful.\n",
    "    good_pos_tag_set = set(['JJ','JJR','JJS', # adjective\n",
    "                        'NN','NNP','NNS','NNPS', # noun\n",
    "                        'RB','RBR','RBS','RP', # adverb and particle\n",
    "                        'VB','VBD','VBG','VBN','VBP','VBZ']) # verb\n",
    "    String = ' '.join(OrderedWord)\n",
    "    word_candidate = OrderedWord\n",
    "    word_final = []\n",
    "    for i in range(len(word_candidate)):\n",
    "        posTag = pos_tag([str(word_candidate[i])])[0][1]\n",
    "        isGood = (good_pos_tag_set.union([posTag])==good_pos_tag_set)\n",
    "        if isGood:\n",
    "            word_final.append(str(word_candidate[i]))\n",
    "    phase_final = extract_candidate_chunks(String, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}')\n",
    "    for p in phase_final:\n",
    "        if (len(tokenizer(p))>4):\n",
    "            phase_final.remove(p)\n",
    "    term_final = set(word_final)|set(phase_final)\n",
    "    return term_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Input a headline list of list where the element refers to\n",
    "a headline in the form of list of its ordered good words.\n",
    "Return a set of vocabulary generated from the input.\n",
    "It takes around 2 minutes to generate a vocabulary for a \n",
    "input contains 1500 days' headlines.\n",
    "'''\n",
    "def getVocabulary(headlines_input):\n",
    "    vocabulary = set([])\n",
    "    for i in range(len(headlines_input)):\n",
    "        for s in headlines_input[i]:\n",
    "            if(type(s)!=list):\n",
    "                continue\n",
    "            else:\n",
    "                vocabulary = vocabulary|buildVocabulary(s)\n",
    "        #if (i%15==0):\n",
    "            #print i/15\n",
    "    final_vocabulary = []\n",
    "    c = 0\n",
    "    for term in vocabulary:\n",
    "        if (len(term.split())<=4):\n",
    "            final_vocabulary.append(term)\n",
    "    final_vocabulary = set(final_vocabulary)\n",
    "    #print len(final_vocabulary)\n",
    "    return final_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Input for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Input a headline list of list where the element refers to\n",
    "a headline in the form of list of its ordered good words.\n",
    "Return a list where the element refers to a day's all good\n",
    "headlines as a whole headline string.\n",
    "'''\n",
    "def toHeadlines(headlines_input):\n",
    "    headlines_output = [0]*len(headlines_input)\n",
    "    for i in range(len(headlines_input)):\n",
    "        headlines_output[i] = []\n",
    "        for s in headlines_input[i]:\n",
    "            if(type(s)==list):\n",
    "                headlines_output[i].append(' '.join(s))\n",
    "        temp = ' '.join(headlines_output[i])\n",
    "        headlines_output[i] = temp\n",
    "    return headlines_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Input a headlines input and a vocabulary.\n",
    "Headlines_input is a headline list of list where the element refers to\n",
    "a headline in the form of list of its ordered good words.\n",
    "Return a tfidf matrix with respect to the vocabulary, but some terms\n",
    "with too low or too high frequency are excluded.\n",
    "'''\n",
    "def getTfidfMat(headlines_input, vocabulary):\n",
    "    headlines_str = toHeadlines(headlines_input)\n",
    "    TFIDF = TfidfVectorizer(min_df=0.03, max_df=0.97, ngram_range=(1, 4))\n",
    "    FitResult = TFIDF.fit(headlines_str)\n",
    "    freq_vocabulary = set(FitResult.get_feature_names())\n",
    "    VOCA = []\n",
    "    for t in freq_vocabulary:\n",
    "        if t in vocabulary:\n",
    "            VOCA.append(t)\n",
    "    TFIDF2 = TfidfVectorizer(vocabulary = VOCA)\n",
    "    tfidf_mat = TFIDF2.fit_transform(headlines_str)\n",
    "    return tfidf_mat,VOCA\n",
    "\n",
    "def getTfidfMat2(headlines_input, vocabulary):\n",
    "    headlines_str = toHeadlines(headlines_input)\n",
    "    TFIDF2 = TfidfVectorizer(vocabulary=vocabulary)\n",
    "    tfidf_mat = TFIDF2.fit_transform(headlines_str)\n",
    "    return tfidf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getInput(headlines_t,DJIA_t,headlines_v,DJIA_v):\n",
    "    VOC = getVocabulary(headlines_t)\n",
    "    HEAD_t = getTfidfMat(headlines_t,VOC)[0]\n",
    "    VOCA = getTfidfMat(headlines_t,VOC)[1]\n",
    "    HEAD_v = getTfidfMat2(headlines_v,VOCA)    \n",
    "    return HEAD_t,DJIA_t,HEAD_v,DJIA_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Seperate the data into training set and test set.\n",
    "And in the training set, seperate ten validation groups.\n",
    "Each group contains a subtraining set of 700 points and a validation set\n",
    "of 100 points.\n",
    "'''\n",
    "headlines = tokenized_df.as_matrix(columns = tokenized_df.columns[1:])\n",
    "DJIA = df.Label[:].values\n",
    "headlines_train = headlines[:1700]\n",
    "headlines_test = headlines[1700:]\n",
    "DJIA_train = df.Label[:1700].values\n",
    "DJIA_test = df.Label[1700:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FORMALset = getInput(headlines_train,DJIA_train,headlines_test,DJIA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CVset = [0]*10\n",
    "CVset[0] = getInput(headlines[:700],DJIA[:700],headlines[700:800],DJIA[700:800])\n",
    "CVset[1] = getInput(headlines[100:800],DJIA[100:800],headlines[800:900],DJIA[800:900])\n",
    "CVset[2] = getInput(headlines[200:900],DJIA[200:900],headlines[900:1000],DJIA[900:1000])\n",
    "CVset[3] = getInput(headlines[300:1000],DJIA[300:1000],headlines[1000:1100],DJIA[1000:1100])\n",
    "CVset[4] = getInput(headlines[400:1100],DJIA[400:1100],headlines[1100:1200],DJIA[1100:1200])\n",
    "CVset[5] = getInput(headlines[500:1200],DJIA[500:1200],headlines[1200:1300],DJIA[1200:1300])\n",
    "CVset[6] = getInput(headlines[600:1300],DJIA[600:1300],headlines[1300:1400],DJIA[1300:1400])\n",
    "CVset[7] = getInput(headlines[700:1400],DJIA[700:1400],headlines[1400:1500],DJIA[1400:1500])\n",
    "CVset[8] = getInput(headlines[800:1500],DJIA[800:1500],headlines[1500:1600],DJIA[1500:1600])\n",
    "CVset[9] = getInput(headlines[900:1600],DJIA[900:1600],headlines[1600:1700],DJIA[1600:1700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<700x369 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 15630 stored elements in Compressed Sparse Row format>,\n",
       "  array([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "         1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "         0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "         0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "         1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "         0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "         1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "         1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "         0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "         1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "         1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "         0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "         1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "         1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "         1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "         1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "         0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "         1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "         0, 1, 1, 1, 0, 1, 0, 0, 0, 1], dtype=int64),\n",
       "  <100x369 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 2909 stored elements in Compressed Sparse Row format>,\n",
       "  array([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "         1, 0, 0, 1, 1, 1, 0, 1], dtype=int64)),\n",
       " (<700x437 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 18518 stored elements in Compressed Sparse Row format>,\n",
       "  array([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "         0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "         1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "         0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "         1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "         1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "         1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "         1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "         1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "         0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "         0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "         1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "         1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "         1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "         0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "         1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1, 1, 1, 0, 1], dtype=int64),\n",
       "  <100x437 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 3393 stored elements in Compressed Sparse Row format>,\n",
       "  array([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "         1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "         1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "         1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "         1, 0, 0, 1, 0, 1, 0, 0], dtype=int64)),\n",
       " (<700x500 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 21677 stored elements in Compressed Sparse Row format>,\n",
       "  array([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "         1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "         1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "         1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "         1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "         1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "         0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "         1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "         1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "         1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "         1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "         0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "         0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "         0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "         1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1, 0, 1, 0, 0], dtype=int64),\n",
       "  <100x500 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 3587 stored elements in Compressed Sparse Row format>,\n",
       "  array([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "         0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 1, 1, 0, 0, 0, 1, 1], dtype=int64)),\n",
       " (<700x531 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 23351 stored elements in Compressed Sparse Row format>,\n",
       "  array([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "         0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "         1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "         1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "         0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "         0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "         0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "         1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "         1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "         1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "         1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "         1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "         0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "         1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "         0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 1, 1, 1, 0, 0, 0, 1, 1], dtype=int64),\n",
       "  <100x531 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 3434 stored elements in Compressed Sparse Row format>,\n",
       "  array([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "         1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "         0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "         1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 1, 1, 0], dtype=int64)),\n",
       " (<700x575 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 25067 stored elements in Compressed Sparse Row format>,\n",
       "  array([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "         0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "         1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "         1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "         1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "         1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "         0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "         1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "         1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "         1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "         0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "         0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "         0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "         0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "         0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "         1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "         1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "         0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "         1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "         0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "         0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "         0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 1, 1, 0], dtype=int64),\n",
       "  <100x575 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 2977 stored elements in Compressed Sparse Row format>,\n",
       "  array([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "         0, 1, 1, 0, 1, 0, 1, 1], dtype=int64)),\n",
       " (<700x572 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 25244 stored elements in Compressed Sparse Row format>,\n",
       "  array([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "         0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "         1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "         1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "         1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "         1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "         1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "         1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "         0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "         0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "         1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "         1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "         0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "         0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "         1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "         1, 1, 0, 1, 1, 0, 1, 0, 1, 1], dtype=int64),\n",
       "  <100x572 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 3339 stored elements in Compressed Sparse Row format>,\n",
       "  array([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "         0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "         1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 1, 0, 0, 1, 0], dtype=int64)),\n",
       " (<700x577 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 25479 stored elements in Compressed Sparse Row format>,\n",
       "  array([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "         1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "         0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "         1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "         0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "         0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "         0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "         0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "         1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "         1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "         1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "         0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "         0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "         1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "         1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "         0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 1, 0, 0, 1, 0], dtype=int64),\n",
       "  <100x577 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 3450 stored elements in Compressed Sparse Row format>,\n",
       "  array([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "         1, 0, 1, 0, 1, 1, 1, 0], dtype=int64)),\n",
       " (<700x597 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 26411 stored elements in Compressed Sparse Row format>,\n",
       "  array([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "         1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "         0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "         1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "         0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "         0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "         1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "         1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "         0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "         1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "         1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "         0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "         1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "         1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "         0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "         1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "         1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "         1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "         0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "         0, 0, 1, 0, 1, 0, 1, 1, 1, 0], dtype=int64),\n",
       "  <100x597 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 3352 stored elements in Compressed Sparse Row format>,\n",
       "  array([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "         0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "         1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "         1, 1, 0, 1, 0, 1, 0, 0], dtype=int64)),\n",
       " (<700x587 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 25777 stored elements in Compressed Sparse Row format>,\n",
       "  array([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "         1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "         1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "         1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "         1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "         0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "         0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "         0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "         0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "         1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "         1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "         1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "         1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "         1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "         1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "         0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "         1, 1, 1, 1, 0, 1, 0, 1, 0, 0], dtype=int64),\n",
       "  <100x587 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 3034 stored elements in Compressed Sparse Row format>,\n",
       "  array([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "         0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "         0, 1, 0, 0, 0, 1, 0, 0], dtype=int64)),\n",
       " (<700x567 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 24536 stored elements in Compressed Sparse Row format>,\n",
       "  array([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "         0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "         1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "         1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "         0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "         1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "         1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "         1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "         0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "         1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "         1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "         1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "         1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "         1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "         0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "         1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "         1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "         0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "         1, 1, 0, 1, 0, 0, 0, 1, 0, 0], dtype=int64),\n",
       "  <100x567 sparse matrix of type '<type 'numpy.float64'>'\n",
       "  \twith 3042 stored elements in Compressed Sparse Row format>,\n",
       "  array([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "         1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "         1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "         0, 1, 1, 0, 0, 1, 1, 0], dtype=int64))]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('FORMALset','wb') as fm:\n",
    "    pickle.dump(FORMALset,fm)\n",
    "with open('FORMALset','rb') as fm:\n",
    "    FORMALset = pickle.load(fm)\n",
    "FORMALset\n",
    "\n",
    "with open('CVset','wb') as cv:\n",
    "    pickle.dump(CVset,cv)\n",
    "with open('CVset','rb') as cv:\n",
    "    CVset = pickle.load(cv)\n",
    "CVset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simulated data: FOR MODEL TEST ONLY!\n",
    "'''\n",
    "# Model test data\n",
    "x1 = np.random.normal(0,1,2000)\n",
    "x2 = np.random.normal(2,5,2000)\n",
    "x3 = np.random.normal(0,4,2000)\n",
    "y = (2*x1+(0.5*x2+3)*x3>5)\n",
    "X = [0]*2000\n",
    "for i in range(2000):\n",
    "    X[i] = [x1[i],x2[i],x3[i]]\n",
    "train_term_document_matrix_tfidf = np.array(X[:1500])\n",
    "test_term_document_matrix_tfidf = np.array(X[1500:])\n",
    "DJIA_train = y[:1500]\n",
    "DJIA_test = y[1500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 Prediction Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bullish(X_train, Y_train, X_test, Y_test):\n",
    "    train_predict = np.ones(len(Y_train))\n",
    "    test_predict = np.ones(len(Y_test))   \n",
    "    accu_train = sum(Y_train == train_predict)/float(len(Y_train))\n",
    "    accu_test = sum(Y_test == test_predict)/float(len(Y_test))\n",
    "    error_train = 1.0-accu_train\n",
    "    error_test = 1.0-accu_test\n",
    "    TP = sum(Y_test*test_predict==1)\n",
    "    TN = sum(Y_test+test_predict==0)\n",
    "    FP = sum(Y_test<test_predict)\n",
    "    FN = sum(Y_test>test_predict)\n",
    "    tpr = TP/float(TP+FN)\n",
    "    fpr = FP/float(FP+TN)\n",
    "    return error_train,error_test,tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bullish: Market All Up\n",
      "Training Error:\t0.46\n",
      "CV Error:\t0.465\n",
      "Test Error:\t0.491349480969\n",
      "Test True Positive Rate:\t1.0\n",
      "Test False Positive Rate:\t1.0\n"
     ]
    }
   ],
   "source": [
    "Bull_CVerror = 0.\n",
    "for i in range(10):\n",
    "    Bull_CVerror = Bull_CVerror+0.1*Bullish(CVset[i][0],CVset[i][1],CVset[i][2],CVset[i][3])[1]\n",
    "Bull = Bullish(FORMALset[0],FORMALset[1],FORMALset[2],FORMALset[3])\n",
    "print \"Bullish: Market All Up\"\n",
    "print \"Training Error:\\t\",Bull[0]\n",
    "print \"CV Error:\\t\",Bull_CVerror\n",
    "print \"Test Error:\\t\",Bull[1]\n",
    "print \"Test True Positive Rate:\\t\",Bull[2]\n",
    "print \"Test False Positive Rate:\\t\",Bull[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "First day has not historical data, give the value 0 as \n",
    "a conservative view on the market.\n",
    "'''\n",
    "def Reversal(X_train, Y_train, X_test, Y_test):\n",
    "    train_predict = 1-np.insert(Y_train[:-1],0,1)\n",
    "    test_predict = 1-np.insert(Y_test[:-1],0,1)\n",
    "    accu_train = sum(Y_train == train_predict)/float(len(Y_train))\n",
    "    accu_test = sum(Y_test == test_predict)/float(len(Y_test))\n",
    "    error_train = 1.0-accu_train\n",
    "    error_test = 1.0-accu_test\n",
    "    TP = sum(Y_test*test_predict==1)\n",
    "    TN = sum(Y_test+test_predict==0)\n",
    "    FP = sum(Y_test<test_predict)\n",
    "    FN = sum(Y_test>test_predict)\n",
    "    tpr = TP/float(TP+FN)\n",
    "    fpr = FP/float(FP+TN)\n",
    "    return error_train,error_test,tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversal: Today down, tomorrow up, vice versa\n",
      "Training Error:\t0.478235294118\n",
      "CV Error:\t0.475\n",
      "Test Error:\t0.467128027682\n",
      "Test True Positive Rate:\t0.52380952381\n",
      "Test False Positive Rate:\t0.457746478873\n"
     ]
    }
   ],
   "source": [
    "Rev_CVerror = 0.\n",
    "for i in range(10):\n",
    "    Rev_CVerror = Rev_CVerror+0.1*Reversal(CVset[i][0],CVset[i][1],CVset[i][2],CVset[i][3])[1]\n",
    "Rev = Reversal(FORMALset[0],FORMALset[1],FORMALset[2],FORMALset[3])\n",
    "print \"Reversal: Today down, tomorrow up, vice versa\"\n",
    "print \"Training Error:\\t\",Rev[0]\n",
    "print \"CV Error:\\t\",Rev_CVerror\n",
    "print \"Test Error:\\t\",Rev[1]\n",
    "print \"Test True Positive Rate:\\t\",Rev[2]\n",
    "print \"Test False Positive Rate:\\t\",Rev[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KNN(X_train, Y_train, X_test, Y_test, K):\n",
    "    \"\"\"\n",
    "    X_train: term_doc_matrix\n",
    "    Y_train: list\n",
    "    X_test: term_doc_matrix\n",
    "    Y_test: list\n",
    "    K: integer\n",
    "    \"\"\"\n",
    "    pred_train = np.zeros(len(Y_train), dtype = 'int64')\n",
    "    pred_test = np.zeros(len(Y_test), dtype = 'int64')\n",
    "    inverted_index_test = { i[0]:i[1] for i in list(zip(range(len(Y_test)),Y_test))}\n",
    "    inverted_index_train= { i[0]:i[1] for i in list(zip(range(len(Y_train)),Y_train))}\n",
    "    sim_score_matrix_K_train = cosine_similarity(X_train, X_train)\n",
    "    for j in range(sim_score_matrix_K_train.shape[0]):\n",
    "        highest_K_score_index_train = np.argsort(sim_score_matrix_K_train[j])[-K:]\n",
    "        DJIA_K_train = [inverted_index_train[idx] for idx in highest_K_score_index_train]\n",
    "        Prob = (sum(DJIA_K_train)+0.)/len(DJIA_K_train) #smoothing\n",
    "        if Prob >= 0.5:\n",
    "            pred_train[j] = 1\n",
    "        else:\n",
    "            pred_train[j] = 0\n",
    "    sim_score_matrix_K =  cosine_similarity(X_test, X_train)\n",
    "    for j in range(sim_score_matrix_K.shape[0]):\n",
    "        highest_K_score_index = np.argsort(sim_score_matrix_K[j])[-K:]\n",
    "        DJIA_K = [inverted_index_train[idx] for idx in highest_K_score_index]\n",
    "        Prob = (sum(DJIA_K)+0.)/len(DJIA_K) #smoothing\n",
    "        if Prob >= 0.5:\n",
    "            pred_test[j] = 1\n",
    "        else:\n",
    "            pred_test[j] = 0\n",
    "    accu_train = np.mean(Y_train == pred_train)        \n",
    "    accu_test = np.mean(Y_test == pred_test)\n",
    "    error_train = 1.0-accu_train\n",
    "    error_test = 1.0-accu_test\n",
    "    TP = sum(Y_test*pred_test==1)\n",
    "    TN = sum(Y_test+pred_test==0)\n",
    "    FP = sum(Y_test<pred_test)\n",
    "    FN = sum(Y_test>pred_test)\n",
    "    tpr = TP/float(TP+FN)\n",
    "    fpr = FP/float(FP+TN)\n",
    "    return error_train,error_test,tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum CV Error: 0.478 \tK = 4\n",
      "KNN Model with K = 4\n",
      "Training Error:\t0.315882352941\n",
      "CV Error:\t0.478\n",
      "Test Error:\t0.519031141869\n",
      "Test True Positive Rate:\t0.727891156463\n",
      "Test False Positive Rate:\t0.774647887324\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBCgUREQk4P8BjQVW9GLXP1QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b7f3240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Knn = [0]*20\n",
    "Knn_CVerror = [0]*20\n",
    "for k in range(20):\n",
    "    Knn_CVerror[k] = 0.\n",
    "    for i in range(10):\n",
    "        Knn_CVerror[k] = Knn_CVerror[k]+0.1*KNN(CVset[i][0],CVset[i][1],CVset[i][2],CVset[i][3],k+1)[1]\n",
    "    Knn[k] = KNN(FORMALset[0],FORMALset[1],FORMALset[2],FORMALset[3],k+1)\n",
    "Ktrain_error =[Knn[k][0] for k in range(20)]\n",
    "one_over_k = [1.0/K for K in range(1, 21)]\n",
    "plt.plot(one_over_k, Ktrain_error, '-yo', label = 'train error')\n",
    "plt.plot(one_over_k, Knn_CVerror, '-go', label = 'CV error')\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('1/K')\n",
    "plt.legend(loc = 'lower left')\n",
    "K_chosen = np.argmin(Knn_CVerror)+1\n",
    "print \"Minimum CV Error:\",Knn_CVerror[K_chosen-1],\"\\tK =\",K_chosen\n",
    "print \"KNN Model with K =\",K_chosen\n",
    "print \"Training Error:\\t\",Knn[K_chosen-1][0]\n",
    "print \"CV Error:\\t\",Knn_CVerror[K_chosen-1]\n",
    "print \"Test Error:\\t\",Knn[K_chosen-1][1]\n",
    "print \"Test True Positive Rate:\\t\",Knn[K_chosen-1][2]\n",
    "print \"Test False Positive Rate:\\t\",Knn[K_chosen-1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def Logit(X_train, Y_train, X_test, Y_test):\n",
    "    logit = LogisticRegression()\n",
    "    logit_fit = logit.fit(X_train, Y_train)\n",
    "    logit_train_predict = logit.predict(X_train)\n",
    "    logit_predict = logit.predict(X_test)\n",
    "    accu_train = sum(Y_train == logit_train_predict)/float(len(Y_train))\n",
    "    accu_test = sum(Y_test == logit_predict)/float(len(Y_test))\n",
    "    error_train = 1.0-accu_train\n",
    "    error_test = 1.0-accu_test\n",
    "    TP = sum(Y_test*logit_predict==1)\n",
    "    TN = sum(Y_test+logit_predict==0)\n",
    "    FP = sum(Y_test<logit_predict)\n",
    "    FN = sum(Y_test>logit_predict)\n",
    "    tpr = TP/float(TP+FN)\n",
    "    fpr = FP/float(FP+TN)\n",
    "    return error_train,error_test,tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Model\n",
      "Training Error:\t0.295294117647\n",
      "CV Error:\t0.493\n",
      "Test Error:\t0.508650519031\n",
      "Test True Positive Rate:\t0.632653061224\n",
      "Test False Positive Rate:\t0.654929577465\n"
     ]
    }
   ],
   "source": [
    "Logit0_CVerror = 0.\n",
    "for i in range(10):\n",
    "    Logit0_CVerror = Logit0_CVerror+0.1*Logit(CVset[i][0],CVset[i][1],CVset[i][2],CVset[i][3])[1]\n",
    "Logit0 = Logit(FORMALset[0],FORMALset[1],FORMALset[2],FORMALset[3])\n",
    "print \"Logistic Model\"\n",
    "print \"Training Error:\\t\",Logit0[0]\n",
    "print \"CV Error:\\t\",Logit0_CVerror\n",
    "print \"Test Error:\\t\",Logit0[1]\n",
    "print \"Test True Positive Rate:\\t\",Logit0[2]\n",
    "print \"Test False Positive Rate:\\t\",Logit0[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogitLasso(X_train, Y_train, X_test, Y_test):\n",
    "    logit = LogisticRegression(penalty='l1')\n",
    "    logit_fit = logit.fit(X_train, Y_train)\n",
    "    logit_train_predict = logit.predict(X_train)\n",
    "    logit_predict = logit.predict(X_test)\n",
    "    accu_train = sum(Y_train == logit_train_predict)/float(len(Y_train))\n",
    "    accu_test = sum(Y_test == logit_predict)/float(len(Y_test))\n",
    "    error_train = 1.0-accu_train\n",
    "    error_test = 1.0-accu_test\n",
    "    TP = sum(Y_test*logit_predict==1)\n",
    "    TN = sum(Y_test+logit_predict==0)\n",
    "    FP = sum(Y_test<logit_predict)\n",
    "    FN = sum(Y_test>logit_predict)\n",
    "    tpr = TP/float(TP+FN)\n",
    "    fpr = FP/float(FP+TN)\n",
    "    return error_train,error_test,tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Lasso Model\n",
      "Training Error:\t0.347058823529\n",
      "CV Error:\t0.455\n",
      "Test Error:\t0.501730103806\n",
      "Test True Positive Rate:\t0.700680272109\n",
      "Test False Positive Rate:\t0.711267605634\n"
     ]
    }
   ],
   "source": [
    "LogitL_CVerror = 0.\n",
    "for i in range(10):\n",
    "    LogitL_CVerror = LogitL_CVerror+0.1*LogitLasso(CVset[i][0],CVset[i][1],CVset[i][2],CVset[i][3])[1]\n",
    "LogitL = LogitLasso(FORMALset[0],FORMALset[1],FORMALset[2],FORMALset[3])\n",
    "print \"Logistic Lasso Model\"\n",
    "print \"Training Error:\\t\",LogitL[0]\n",
    "print \"CV Error:\\t\",LogitL_CVerror\n",
    "print \"Test Error:\\t\",LogitL[1]\n",
    "print \"Test True Positive Rate:\\t\",LogitL[2]\n",
    "print \"Test False Positive Rate:\\t\",LogitL[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogitRidge(X_train, Y_train, X_test, Y_test):\n",
    "    logit = LogisticRegression(penalty='l2')\n",
    "    logit_fit = logit.fit(X_train, Y_train)\n",
    "    logit_train_predict = logit.predict(X_train)\n",
    "    logit_predict = logit.predict(X_test)\n",
    "    accu_train = sum(Y_train == logit_train_predict)/float(len(Y_train))\n",
    "    accu_test = sum(Y_test == logit_predict)/float(len(Y_test))\n",
    "    error_train = 1.0-accu_train\n",
    "    error_test = 1.0-accu_test\n",
    "    TP = sum(Y_test*logit_predict==1)\n",
    "    TN = sum(Y_test+logit_predict==0)\n",
    "    FP = sum(Y_test<logit_predict)\n",
    "    FN = sum(Y_test>logit_predict)\n",
    "    tpr = TP/float(TP+FN)\n",
    "    fpr = FP/float(FP+TN)\n",
    "    return error_train,error_test,tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Ridge Model\n",
      "Training Error:\t0.295294117647\n",
      "CV Error:\t0.493\n",
      "Test Error:\t0.508650519031\n",
      "Test True Positive Rate:\t0.632653061224\n",
      "Test False Positive Rate:\t0.654929577465\n"
     ]
    }
   ],
   "source": [
    "LogitR_CVerror = 0.\n",
    "for i in range(10):\n",
    "    LogitR_CVerror = LogitR_CVerror+0.1*LogitRidge(CVset[i][0],CVset[i][1],CVset[i][2],CVset[i][3])[1]\n",
    "LogitR = LogitRidge(FORMALset[0],FORMALset[1],FORMALset[2],FORMALset[3])\n",
    "print \"Logistic Ridge Model\"\n",
    "print \"Training Error:\\t\",LogitR[0]\n",
    "print \"CV Error:\\t\",LogitR_CVerror\n",
    "print \"Test Error:\\t\",LogitR[1]\n",
    "print \"Test True Positive Rate:\\t\",LogitR[2]\n",
    "print \"Test False Positive Rate:\\t\",LogitR[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Rocchio Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Rocchio(X_train, Y_train, X_test, Y_test):\n",
    "    pred_train = np.zeros(len(Y_train), dtype = 'int64')\n",
    "    pred_test = np.zeros(len(Y_test), dtype = 'int64')\n",
    "    X_train_1 = X_train[np.array(Y_train) == 1, :]\n",
    "    X_train_0 = X_train[np.array(Y_train) == 0, :]\n",
    "    Centriod_1 = (np.sum(X_train_1, axis = 0)+0.)/X_train_1.shape[0]\n",
    "    Centriod_0 = (np.sum(X_train_0, axis = 0)+0.)/X_train_0.shape[0]\n",
    "    for j in range(len(Y_train)):\n",
    "        distance_1 = np.sqrt(np.sum(np.square(X_train[j]-Centriod_1)))\n",
    "        distance_0 = np.sqrt(np.sum(np.square(X_train[j]-Centriod_0)))\n",
    "        if distance_1 <= distance_0:\n",
    "            pred_train[j] = 1\n",
    "        else:\n",
    "            pred_train[j] = 0\n",
    "    accu_train = np.mean(Y_train == pred_train)\n",
    "    for j in range(len(Y_test)):\n",
    "        distance_1 = np.sqrt(np.sum(np.square(X_test[j]-Centriod_1)))\n",
    "        distance_0 = np.sqrt(np.sum(np.square(X_test[j]-Centriod_0)))\n",
    "        if distance_1 <= distance_0:\n",
    "            pred_test[j] = 1\n",
    "        else:\n",
    "            pred_test[j] = 0\n",
    "    accu_test = np.mean(Y_test == pred_test)\n",
    "    error_train = 1.0-accu_train\n",
    "    error_test = 1.0-accu_test   \n",
    "    TP = sum(Y_test*pred_test==1)\n",
    "    TN = sum(Y_test+pred_test==0)\n",
    "    FP = sum(Y_test<pred_test)\n",
    "    FN = sum(Y_test>pred_test)\n",
    "    tpr = TP/float(TP+FN)\n",
    "    fpr = FP/float(FP+TN)\n",
    "    return error_train,error_test,tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rocchio Classification Model\n",
      "Training Error:\t0.319411764706\n",
      "CV Error:\t0.504\n",
      "Test Error:\t0.512110726644\n",
      "Test True Positive Rate:\t0.517006802721\n",
      "Test False Positive Rate:\t0.542253521127\n"
     ]
    }
   ],
   "source": [
    "RC_CVerror = 0.\n",
    "for i in range(10):\n",
    "    RC_CVerror = RC_CVerror+0.1*Rocchio(CVset[i][0],CVset[i][1],CVset[i][2],CVset[i][3])[1]\n",
    "RC = Rocchio(FORMALset[0],FORMALset[1],FORMALset[2],FORMALset[3])\n",
    "print \"Rocchio Classification Model\"\n",
    "print \"Training Error:\\t\",RC[0]\n",
    "print \"CV Error:\\t\",RC_CVerror\n",
    "print \"Test Error:\\t\",RC[1]\n",
    "print \"Test True Positive Rate:\\t\",RC[2]\n",
    "print \"Test False Positive Rate:\\t\",RC[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "def NaiveBayes(X_train, Y_train, X_test, Y_test):\n",
    "    BNBclassifier = BernoulliNB(alpha=1)\n",
    "    BNBclassifier.fit(X_train, Y_train)\n",
    "    BNBpred_train = BNBclassifier.predict(X_train)\n",
    "    BNBpred = BNBclassifier.predict(X_test)\n",
    "    accu_train = np.mean(BNBpred_train==Y_train)\n",
    "    accu_test = np.mean(BNBpred==Y_test)\n",
    "    error_train = 1.0-accu_train\n",
    "    error_test = 1.0-accu_test\n",
    "    TP = sum(Y_test*BNBpred==1)\n",
    "    TN = sum(Y_test+BNBpred==0)\n",
    "    FP = sum(Y_test<BNBpred)\n",
    "    FN = sum(Y_test>BNBpred)\n",
    "    tpr = TP/float(TP+FN)\n",
    "    fpr = FP/float(FP+TN)\n",
    "    return error_train,error_test,tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model\n",
      "Training Error:\t0.311764705882\n",
      "CV Error:\t0.526\n",
      "Test Error:\t0.508650519031\n",
      "Test True Positive Rate:\t0.551020408163\n",
      "Test False Positive Rate:\t0.570422535211\n"
     ]
    }
   ],
   "source": [
    "NB_CVerror = 0.\n",
    "for i in range(10):\n",
    "    NB_CVerror = NB_CVerror+0.1*NaiveBayes(CVset[i][0],CVset[i][1],CVset[i][2],CVset[i][3])[1]\n",
    "NB = NaiveBayes(FORMALset[0],FORMALset[1],FORMALset[2],FORMALset[3])\n",
    "print \"Naive Bayes Model\"\n",
    "print \"Training Error:\\t\",NB[0]\n",
    "print \"CV Error:\\t\",NB_CVerror\n",
    "print \"Test Error:\\t\",NB[1]\n",
    "print \"Test True Positive Rate:\\t\",NB[2]\n",
    "print \"Test False Positive Rate:\\t\",NB[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REFERENCE: http://scikit-learn.org/stable/modules/tree.html\n",
    "from sklearn import tree\n",
    "def DecisionTree(X_train, Y_train, X_test, Y_test):\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    clf_train = clf.predict(X_train)\n",
    "    clf_test = clf.predict(X_test)\n",
    "    accu_train = sum(Y_train == clf_train)/float(len(Y_train))\n",
    "    accu_test = sum(Y_test == clf_test)/float(len(Y_test))\n",
    "    error_train = 1.0-accu_train\n",
    "    error_test = 1.0-accu_test\n",
    "    TP = sum(Y_test*clf_test==1)\n",
    "    TN = sum(Y_test+clf_test==0)\n",
    "    FP = sum(Y_test<clf_test)\n",
    "    FN = sum(Y_test>clf_test)\n",
    "    tpr = TP/float(TP+FN)\n",
    "    fpr = FP/float(FP+TN)\n",
    "    return error_train,error_test,tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model\n",
      "Training Error:\t0.00117647058824\n",
      "CV Error:\t0.525\n",
      "Test Error:\t0.529411764706\n",
      "Test True Positive Rate:\t0.557823129252\n",
      "Test False Positive Rate:\t0.619718309859\n"
     ]
    }
   ],
   "source": [
    "DT_CVerror = 0.\n",
    "for i in range(10):\n",
    "    DT_CVerror = DT_CVerror+0.1*DecisionTree(CVset[i][0],CVset[i][1],CVset[i][2],CVset[i][3])[1]\n",
    "DT = DecisionTree(FORMALset[0],FORMALset[1],FORMALset[2],FORMALset[3])\n",
    "print \"Naive Bayes Model\"\n",
    "print \"Training Error:\\t\",DT[0]\n",
    "print \"CV Error:\\t\",DT_CVerror\n",
    "print \"Test Error:\\t\",DT[1]\n",
    "print \"Test True Positive Rate:\\t\",DT[2]\n",
    "print \"Test False Positive Rate:\\t\",DT[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REFERENCE: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RandomForest(X_train, Y_train, X_test, Y_test, md):\n",
    "    clfr = RandomForestClassifier(max_depth=md, random_state=0)\n",
    "    clfr = clfr.fit(X_train, Y_train)\n",
    "    clfr_train = clfr.predict(X_train)\n",
    "    clfr_test = clfr.predict(X_test)\n",
    "    accu_train = sum(Y_train == clfr_train)/float(len(Y_train))\n",
    "    accu_test = sum(Y_test == clfr_test)/float(len(Y_test))\n",
    "    error_train = 1.0-accu_train\n",
    "    error_test = 1.0-accu_test\n",
    "    TP = sum(Y_test*clfr_test==1)\n",
    "    TN = sum(Y_test+clfr_test==0)\n",
    "    FP = sum(Y_test<clfr_test)\n",
    "    FN = sum(Y_test>clfr_test)\n",
    "    tpr = TP/float(TP+FN)\n",
    "    fpr = FP/float(FP+TN)\n",
    "    return error_train,error_test,tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum CV Error: 0.468 \t max_depth = 1\n",
      "Random Forests Model with K = 1\n",
      "Training Error:\t0.458823529412\n",
      "CV Error:\t0.468\n",
      "Test Error:\t0.498269896194\n",
      "Test True Positive Rate:\t0.979591836735\n",
      "Test False Positive Rate:\t0.992957746479\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b2cd780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RF = [0]*50\n",
    "RF_CVerror = [0]*50\n",
    "for md in range(50):\n",
    "    RF_CVerror[md] = 0.\n",
    "    for i in range(10):\n",
    "        RF_CVerror[md] = RF_CVerror[md]+0.1*RandomForest(CVset[i][0],CVset[i][1],CVset[i][2],CVset[i][3],md+1)[1]\n",
    "    RF[md] = RandomForest(FORMALset[0],FORMALset[1],FORMALset[2],FORMALset[3],md+1)\n",
    "RFtrain_error =[RF[md][0] for md in range(50)]\n",
    "plt.plot(range(1, 51), RFtrain_error, '-yo', label = 'train error')\n",
    "plt.plot(range(1, 51), RF_CVerror, '-go', label = 'CV error')\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('max depth')\n",
    "plt.legend(loc = 'lower left')\n",
    "MD_chosen = np.argmin(RF_CVerror)+1\n",
    "print \"Minimum CV Error:\",RF_CVerror[MD_chosen-1],\"\\t max_depth =\",MD_chosen\n",
    "print \"Random Forests Model with K =\",MD_chosen\n",
    "print \"Training Error:\\t\",RF[MD_chosen-1][0]\n",
    "print \"CV Error:\\t\",RF_CVerror[MD_chosen-1]\n",
    "print \"Test Error:\\t\",RF[MD_chosen-1][1]\n",
    "print \"Test True Positive Rate:\\t\",RF[MD_chosen-1][2]\n",
    "print \"Test False Positive Rate:\\t\",RF[MD_chosen-1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
